{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a continuation of 1_allele.ipynb and shows how the functions work in case of multiple alleles and splits, within which multiple training runs are performed.\n",
    "\n",
    "\n",
    "*Data Functions (data_funcs) – Contains rewritten and extended functions for data preprocessing, feature extraction, and dataset handling to ensure compatibility with the HMM structure.*\n",
    "\n",
    "*Logic (logic) – Implements custom HMM-related algorithms, state transitions, and probability computations, refining or replacing certain pomegranate functions as needed.*\n",
    "\n",
    "*Visualization (visualisation) – Provides tailored plotting and analysis tools to interpret model performance, state sequences, and emission distributions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pomegranate.io\n",
    "from pomegranate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx import all_simple_paths\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "PATH_TO_PREDICTOR_HOME = \"../..\"\n",
    "sys.path.append(PATH_TO_PREDICTOR_HOME)\n",
    "METHOD = \"hmm_pomegranate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from peptides_utils import split_to_dicts, join_dicts, get_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hmm_visualization_methods import *\n",
    "from training_parameters import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_reading_methods import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import training_parameters\n",
    "from peptides_utils import defineClass, defineOrganism, make_logo_for_data\n",
    "from data_reading_methods import remove_unused_lengths, transform_data_to_properties_and_join_alleles, calculate_weights_based_on_length_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logomaker\n",
    "from collections import deque\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import jensenshannon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(experiment_params :ExperimentParams, type_of_peptides= \"binders\"):\n",
    "    data_params : DataScenarioParams = experiment_params.data_scenario_params\n",
    "    data_scenario = data_params.data_scenario\n",
    "    DATA_PATH = data_params.input_data_path\n",
    "    TOTAL_SPLITS = data_params.splits_to_read\n",
    "    print(f\"Will read files from the folder {DATA_PATH}\")\n",
    "    assert data_scenario in [\"IEDB_preprocessed\", \"simulated\", \"simulated_preprocessed\", 'MixMHCpred']\n",
    "    additional_return = list()\n",
    "    if isinstance(data_params, PreprocessedIEDBDataParams):\n",
    "        ALLELES = get_available_alleles(DATA_PATH)\n",
    "        per_allele_per_kfold_per_length_binders_train = read_data(DATA_PATH,ALLELES, \"train\", type_of_peptides)\n",
    "        per_allele_per_kfold_per_length_binders_test = read_data(DATA_PATH,ALLELES, \"test\", type_of_peptides)\n",
    "        sample_allele = list(per_allele_per_kfold_per_length_binders_train.keys())[0]\n",
    "        per_allele_df = join_dicts(per_allele_per_kfold_per_length_binders_train)\n",
    "        for allele_name in ALLELES:\n",
    "            per_allele_df[allele_name]['allele'] = allele_name\n",
    "        assert len(per_allele_per_kfold_per_length_binders_train[sample_allele].keys()) >= TOTAL_SPLITS # check number of splits\n",
    "        additional_return.append(per_allele_df)\n",
    "    elif isinstance(data_params, SimulatedPreprocessedDataParams):\n",
    "        ALLELES = get_available_alleles(DATA_PATH, do_not_parse_alleles=True)\n",
    "        per_allele_per_kfold_per_length_binders_train = read_data(DATA_PATH,ALLELES, \"train\", type_of_peptides, do_not_parse_alleles=True)\n",
    "        per_allele_per_kfold_per_length_binders_test = read_data(DATA_PATH,ALLELES, \"test\", type_of_peptides,do_not_parse_alleles=True)\n",
    "        sample_allele = list(per_allele_per_kfold_per_length_binders_train.keys())[0]\n",
    "        per_allele_df = join_dicts(per_allele_per_kfold_per_length_binders_train)\n",
    "        for allele_name in ALLELES:\n",
    "            per_allele_df[allele_name]['allele'] = allele_name\n",
    "        assert len(per_allele_per_kfold_per_length_binders_train[sample_allele].keys()) >= TOTAL_SPLITS # check number of splits\n",
    "        additional_return.append(per_allele_df)\n",
    "    elif isinstance(data_params, SimulatedDataParams):\n",
    "        simulated_exact_file = data_params.simulated_exact_file_name\n",
    "        dummy_allele_name = data_params.dummy_allele_name\n",
    "        simulated_scenario = data_params.simulated_scenario\n",
    "        SIMULATED_DATA_PATH = f\"{DATA_PATH}/{simulated_scenario}/{simulated_exact_file}\"\n",
    "        ALLELES = [dummy_allele_name]\n",
    "        per_allele_df = dict()\n",
    "        # For now just read the same data multiple times for alleles/splits\n",
    "        for allele_name in ALLELES:\n",
    "            allele_df = pd.read_csv(SIMULATED_DATA_PATH, sep=\";\")\n",
    "            list_dfs = [allele_df.copy() for i in range(TOTAL_SPLITS)]\n",
    "            for split_num, split_df in enumerate(list_dfs):\n",
    "                split_df['split'] = split_num\n",
    "                split_df['allele_name'] = allele_name\n",
    "                result_allele_df = pd.concat(list_dfs)\n",
    "            per_allele_df[allele_name] = result_allele_df\n",
    "            result_allele_df['length'] = split_df.peptide.str.len()\n",
    "            TARGET_LENGTHS = list(split_df['length'].unique())\n",
    "        # split data into dicts\n",
    "        per_allele_per_kfold_per_length_binders_train = split_to_dicts(per_allele_df,\n",
    "                                                                  ALLELES=ALLELES,\n",
    "                                                                  TARGET_LENGTHS=TARGET_LENGTHS,\n",
    "                                                                  TOTAL_SPLITS=np.arange(TOTAL_SPLITS))\n",
    "        per_allele_per_kfold_per_length_binders_test =  split_to_dicts(per_allele_df,\n",
    "                                                                  ALLELES=ALLELES,\n",
    "                                                                  TARGET_LENGTHS=TARGET_LENGTHS,\n",
    "                                                                  TOTAL_SPLITS=np.arange(TOTAL_SPLITS))\n",
    "        additional_return.append(per_allele_df)\n",
    "    elif isinstance(data_params, MixMHCpredDataParams):\n",
    "        mixture_name = data_params.mixmhc_mixture_name\n",
    "        dummy_allele_name = data_params.dummy_allele_name\n",
    "        df = pd.read_csv(DATA_PATH, sep=';')\n",
    "        print(df.columns)\n",
    "        df = df.loc[\n",
    "            df.Peptide.str.match(\"^[ACDEFGHIKLMNPQRSTVWY]+$\")\n",
    "        ]\n",
    "        print(\"Total table length\", len(df))\n",
    "        #Filter out selected mixture\n",
    "        df = df.loc[df.Sample_IDs.str.split(', ').apply(lambda x: mixture_name in x),]\n",
    "        print(\"Filtered for given mixture\", len(df))\n",
    "        sample_data = pd.DataFrame(\n",
    "            {\"peptide\": df.Peptide.values,\n",
    "             \"old_sample_id\": df.Sample_IDs,\n",
    "             \"sample_id\": mixture_name,\n",
    "             \"mixmhc_predicted_mixed_alleles\": df.Allele.values})\n",
    "\n",
    "        list_dfs = [sample_data.copy() for i in range(TOTAL_SPLITS)]\n",
    "        per_allele_df = dict()\n",
    "        allele_name = \"d_\" + dummy_allele_name\n",
    "        ALLELES = [allele_name]\n",
    "        for allele_name in ALLELES:\n",
    "            for split_num, split_df in enumerate(list_dfs):\n",
    "                split_df['split'] = split_num\n",
    "                split_df['allele'] = allele_name\n",
    "                split_df['length'] = split_df.peptide.str.len()\n",
    "            result_allele_df = pd.concat(list_dfs)\n",
    "            result_allele_df = result_allele_df.drop_duplicates(subset=['peptide'])\n",
    "            TARGET_LENGTHS = list(split_df['length'].unique())\n",
    "            per_allele_df[allele_name] = result_allele_df\n",
    "        per_allele_per_kfold_per_length_binders_train = split_to_dicts(per_allele_df,\n",
    "                                                                  ALLELES=ALLELES,\n",
    "                                                                  TARGET_LENGTHS=TARGET_LENGTHS,\n",
    "                                                                  TOTAL_SPLITS=np.arange(TOTAL_SPLITS))\n",
    "        per_allele_per_kfold_per_length_binders_test = split_to_dicts(per_allele_df,\n",
    "                                                                  ALLELES=ALLELES,\n",
    "                                                                  TARGET_LENGTHS=TARGET_LENGTHS,\n",
    "                                                                  TOTAL_SPLITS=np.arange(TOTAL_SPLITS))\n",
    "        additional_return.append(per_allele_df)\n",
    "        additional_return.append(df)\n",
    "    return per_allele_per_kfold_per_length_binders_train,  per_allele_per_kfold_per_length_binders_test, additional_return\n",
    "\n",
    "from pomegranate.io import BatchedDataGenerator, SequenceGenerator\n",
    "def create_char_arrays(peptide_sequences):\n",
    "    return np.array([[char for char in peptide] for peptide in peptide_sequences], dtype=object)\n",
    "\n",
    "def prepare_split_data_separeted_length(per_length_data, per_length_weights, per_length_test_data, target_lengths):\n",
    "    binders_array = np.array([per_length_data[length][i] for length in target_lengths\n",
    "                              for i in range(len(per_length_data[length]))], dtype=object)\n",
    "    weights_array = np.array([per_length_weights[length][i] for length in target_lengths\n",
    "                               for i in range(len(per_length_weights[length]))], dtype=object)\n",
    "    binders_test_array = np.array([per_length_test_data[length][i] for length in target_lengths\n",
    "                                   for i in range(len(per_length_test_data[length]))], dtype=object)\n",
    "    return binders_array, weights_array, binders_test_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## logic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_models_for_runs(allele_name, experiment_params: ExperimentParams, split_num = 1, custom_models_id: str = \"root\"):\n",
    "    model_training_params = experiment_params.model_training_params\n",
    "    original_num_runs = model_training_params.num_runs\n",
    "    total_runs = original_num_runs * model_training_params.decrease_anchor_aas_steps\n",
    "    \n",
    "\n",
    "    target_allele_name = allele_name.replace('-', '_').replace('*', '_').replace(':', '_').replace('/', '_')\n",
    "    models_for_runs = {}\n",
    "    \n",
    "    for run_num in range(total_runs):\n",
    "        run_index = f\"{custom_models_id}[{run_num:04d}]\"\n",
    "        acids_to_subtract = run_num // original_num_runs\n",
    "        current_params = deepcopy(model_training_params)\n",
    "        current_params.anchor_top_aas -= acids_to_subtract\n",
    "        \n",
    "        prepared_model = build_model_based_on_params(current_params)\n",
    "        prepared_model.name = (\n",
    "            f'{run_index}_run-{current_params.get_model_common_names()}_model-{target_allele_name}-{split_num}'\n",
    "        )\n",
    "        \n",
    "        models_for_runs[run_index] = prepared_model\n",
    "    \n",
    "    return models_for_runs\n",
    "\n",
    "def prepare_multiple_models(experiment_params: ExperimentParams):\n",
    "    data_scenario_params = experiment_params.data_scenario_params\n",
    "    model_training_params = experiment_params.model_training_params\n",
    "    alleles_to_use = model_training_params.alleles_to_use\n",
    "\n",
    "    per_allele_per_split_prepared_models = {}\n",
    "    \n",
    "    for allele_name in alleles_to_use:\n",
    "        per_allele_per_split_prepared_models[allele_name] = {}\n",
    "        \n",
    "        for split_num in range(data_scenario_params.splits_to_read):\n",
    "            per_allele_per_split_prepared_models[allele_name][split_num] = make_models_for_runs(\n",
    "                allele_name, experiment_params, split_num\n",
    "            )\n",
    "    \n",
    "    return per_allele_per_split_prepared_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_char_arrays(peptide_sequences):\n",
    "    return np.array([[char for char in peptide] for peptide in peptide_sequences], dtype=object)\n",
    "\n",
    "def prepare_split_data_separeted_length(per_length_data, per_length_weights, per_length_test_data, target_lengths):\n",
    "    binders_array = np.array([per_length_data[length][i] for length in target_lengths\n",
    "                              for i in range(len(per_length_data[length]))], dtype=object)\n",
    "    weights_array = np.array([per_length_weights[length][i] for length in target_lengths\n",
    "                               for i in range(len(per_length_weights[length]))], dtype=object)\n",
    "    binders_test_array = np.array([per_length_test_data[length][i] for length in target_lengths\n",
    "                                   for i in range(len(per_length_test_data[length]))], dtype=object)\n",
    "    return binders_array, weights_array, binders_test_array\n",
    "\n",
    "def process_split_data(per_length_data, per_length_weights, per_length_test_data, target_lengths):\n",
    "    \"\"\"Prepare and shuffle data arrays.\"\"\"\n",
    "    binders_array, weights_array, binders_test_array = prepare_split_data_separeted_length(per_length_data, per_length_weights, per_length_test_data, target_lengths)\n",
    "    rng = np.random.default_rng()\n",
    "    new_indexes = rng.permutation(len(binders_array))\n",
    "    binders_array = binders_array[new_indexes]\n",
    "    weights_array = weights_array[new_indexes]\n",
    "    rng.shuffle(binders_test_array)\n",
    "    sample_X = create_char_arrays(binders_array)\n",
    "    sample_X_test = create_char_arrays(binders_test_array)\n",
    "    return sample_X, sample_X_test, weights_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_model( model, sample_X, sample_X_test, weights_array, model_training_params):\n",
    "    \"\"\"Fit a single model for a given split.\"\"\"\n",
    "    verbose = model_training_params.verbose\n",
    "    multiple_check_input = model_training_params.multiple_check_inpit\n",
    "    algorithm = model_training_params.algorithm\n",
    "    lr_decay = model_training_params.lr_decay\n",
    "    minibatch_training = model_training_params.minibatch_training\n",
    "    batches_per_epoch = model_training_params.batches_per_epoch\n",
    "    batch_size = model_training_params.batch_size\n",
    "    min_iterations = model_training_params.min_iters\n",
    "    max_iterations = model_training_params.maxiters\n",
    "    emission_pseudocount = model_training_params.emission_pseudocount\n",
    "    transition_pseudocount = model_training_params.transition_pseudocount\n",
    "    use_pseudocount = model_training_params.use_pseudocounts\n",
    "    edge_inertia = model_training_params.edge_inertia\n",
    "    distribution_inertia = model_training_params.distribution_inertia\n",
    "    stop_threshold = model_training_params.stop_threshold\n",
    "    n_jobs = 1  # Modify if needed\n",
    "    \n",
    "    sequence_test_generator = SequenceGenerator(sample_X_test)\n",
    "    \n",
    "    if minibatch_training:\n",
    "        sequence_generator = BatchedDataGenerator(sample_X,\n",
    "                                                  batches_per_epoch=batches_per_epoch,\n",
    "                                                  batch_size=batch_size)\n",
    "        sequence_generator.reset()\n",
    "    else:\n",
    "        sequence_generator = sample_X\n",
    "    \n",
    "    model, history = model.fit(sequence_generator,\n",
    "                               sequences_test=sequence_test_generator,\n",
    "                               stop_threshold=stop_threshold,\n",
    "                               return_history=True,\n",
    "                               verbose=verbose,\n",
    "                               multiple_check_input=multiple_check_input,\n",
    "                               n_jobs=n_jobs,\n",
    "                               algorithm=algorithm,\n",
    "                               lr_decay=lr_decay,\n",
    "                               distribution_inertia=distribution_inertia,\n",
    "                               edge_inertia=edge_inertia,\n",
    "                               batches_per_epoch=batches_per_epoch,\n",
    "                               min_iterations=min_iterations,\n",
    "                               max_iterations=max_iterations,\n",
    "                               emission_pseudocount=emission_pseudocount,\n",
    "                               transition_pseudocount=transition_pseudocount,\n",
    "                               use_pseudocount=use_pseudocount,\n",
    "                               weights=weights_array)\n",
    "    \n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models_for_runs(prepared_models, train_data, test_data, train_data_weights, model_training_params, path_to_save_models):\n",
    "    \"\"\"\n",
    "    Train models for different runs within a single split.\n",
    "    \"\"\"\n",
    "    result_models = {}\n",
    "    result_histories = {}\n",
    "    target_lengths = model_training_params.lengths_to_use\n",
    "\n",
    "    per_run_models = dict()\n",
    "    per_run_histories = dict()\n",
    "    \n",
    "    for run_index, model in prepared_models.items():\n",
    "        print(f\"Run {run_index}\", end=\" \")\n",
    "        if model and run_index in prepared_models:\n",
    "            sample_X, sample_X_test, weights_array = process_split_data(train_data, train_data_weights, test_data, target_lengths)\n",
    "            model, history = train_single_model(model, sample_X, sample_X_test, weights_array, model_training_params)\n",
    "            per_run_models[run_index] = model\n",
    "            per_run_histories[run_index] = history\n",
    "            \n",
    "            path_to_save_models_runs = path_to_save_models \n",
    "            save_model(path_to_save_models_runs, model, history)\n",
    "        else:\n",
    "            print(f\"Skipping training for split {run_index}, model not provided.\")\n",
    "            break\n",
    "    \n",
    "    print(\" \")\n",
    "    return per_run_models, per_run_histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models_for_splits(prepared_models, train_data, test_data, train_data_weights, model_training_params, experiment_params, path_to_save_models):\n",
    "    \"\"\"\n",
    "    Train models for different splits within a single allele.\n",
    "    \"\"\"\n",
    "    result_models = {}\n",
    "    result_histories = {}\n",
    "    \n",
    "    for split_num, split_models in prepared_models.items():\n",
    "        print(f\"Split {split_num}\", end=\" \")\n",
    "        split_path = f\"{path_to_save_models}/s{split_num}/\"\n",
    "        split_models_trained, split_histories = train_models_for_runs(\n",
    "            prepared_models=split_models,\n",
    "            train_data=train_data[split_num],\n",
    "            test_data=test_data[split_num],\n",
    "            train_data_weights=train_data_weights[split_num],\n",
    "            model_training_params=model_training_params,\n",
    "            path_to_save_models=split_path,\n",
    "        )\n",
    "\n",
    "\n",
    "        if model_training_params.check_model_integrity:\n",
    "            models_to_be_updated = check_models_integrity_and_shift_params(split_models_trained, train_data, experiment_params=experiment_params)\n",
    "            if (models_to_be_updated):\n",
    "                print(\"some models had shifts, we will try one more iteration of training\")\n",
    "                extra_models, extra_histories = train_models_for_runs(\n",
    "                    prepared_models=split_models,\n",
    "                    train_data=train_data[split_num],\n",
    "                    test_data=test_data[split_num],\n",
    "                    train_data_weights=train_data_weights[split_num],\n",
    "                    model_training_params=model_training_params,\n",
    "                    path_to_save_models=split_path,\n",
    "                )\n",
    "                split_models_trained.update(extra_models)\n",
    "                split_histories.update(extra_histories)\n",
    "\n",
    "        result_models[split_num] = split_models_trained\n",
    "        result_histories[split_num] = split_histories    \n",
    "    return result_models, result_histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models_for_alleles(prepared_models, train_data, test_data, train_data_weights, experiment_params: ExperimentParams, subfolder_to_save_result: str):\n",
    "    \"\"\"\n",
    "    Train models for different alleles, iterating over splits first.\n",
    "    \"\"\"\n",
    "    model_training_params = experiment_params.model_training_params\n",
    "    path_to_save_models = f\"{experiment_params.experiment_result_data_path}/{subfolder_to_save_result}/\"\n",
    "    result_models = dict()\n",
    "    result_histories = dict()\n",
    "    \n",
    "    for allele_name in train_data.keys():\n",
    "        print(f\"Allele {allele_name}:\", end=\" \")\n",
    "        result_models[allele_name] = dict()\n",
    "        result_histories[allele_name] = dict()\n",
    "        target_allele_name = allele_name.replace('-', '_').replace('*', '_').replace(':', '_')\n",
    "        path_to_save_models_for_allele = f\"{path_to_save_models}/{target_allele_name}/\"\n",
    "        \n",
    "        allele_models, allele_histories = train_models_for_splits(\n",
    "            prepared_models=prepared_models[allele_name],\n",
    "            train_data=train_data[allele_name],\n",
    "            test_data=test_data[allele_name],\n",
    "            train_data_weights=train_data_weights[allele_name],\n",
    "            model_training_params=model_training_params,\n",
    "            experiment_params=experiment_params,\n",
    "            path_to_save_models=path_to_save_models_for_allele\n",
    "        )\n",
    "        \n",
    "        result_models[allele_name] = allele_models\n",
    "        result_histories[allele_name] = allele_histories\n",
    "    \n",
    "    return result_models, result_histories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visoalisation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def save_visualizations_for_model(model, history, split_path, experiment_params, predefined_hierarchical_layout):\n",
    "    \"\"\"\n",
    "    Saves various visualizations for a given model, including the learning curve,\n",
    "    state graph, distributions, and PyViz graph.\n",
    "    \"\"\"\n",
    "    path_for_model = f\"{split_path}/{model.name}/\"\n",
    "    if not os.path.exists(path_for_model):\n",
    "        os.makedirs(path_for_model)\n",
    "    \n",
    "\n",
    "    # Learning Curve\n",
    "    path_to_save_learning_curve = path_for_model + \"LearningCurve\"\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    sns.lineplot(history.log_probabilities, ax=ax)\n",
    "    plt.savefig(path_to_save_learning_curve)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # State Graph\n",
    "    #print('ModelGraph', end = ' ')\n",
    "    path_to_save_ModelGraph = path_for_model + \"ModelGraph.png\"\n",
    "\n",
    "    with open(path_to_save_ModelGraph, 'w+') as f:\n",
    "        model.plot(file=f, crop_zero=True)\n",
    "\n",
    "    # Distributions\n",
    "    #print('Distributions', end = ' ')\n",
    "    plot_distributions_for_states(model, split_path, horizontal=False,\n",
    "                                  discrete=experiment_params.model_training_params.aa_labels_training,\n",
    "                                  initial_params=experiment_params.model_training_params.initial_params)\n",
    "\n",
    "    # PyViz Graph\n",
    "    #print('PyViz', end = ' ')\n",
    "    make_pyviz_graph(model, split_path, precision=3,\n",
    "                     prefefined_hierarchical_layout=predefined_hierarchical_layout)\n",
    "    \n",
    "def save_visualizations_for_runs(run_model, history_per_run, experiment_params, split_path, predefined_hierarchical_layout=True):\n",
    "    for run_index, model in run_model.items():\n",
    "        history = history_per_run[run_index]\n",
    "        save_visualizations_for_model(model, history, split_path, \n",
    "                                    experiment_params, predefined_hierarchical_layout)\n",
    "\n",
    "def save_visualizations_for_multiple_models(per_allele_new_models, per_allele_histories, \n",
    "                                           experiment_params, subfolder_to_safe_result,\n",
    "                                           predefined_hierarchical_layout=True):\n",
    "    \"\"\"\n",
    "    Processes the models in the hierarchical structure and calls save_visualizations_for_model\n",
    "    for each one found in the nested dictionaries.\n",
    "    \"\"\"\n",
    "    base_path = f\"{experiment_params.experiment_result_data_path}/{subfolder_to_safe_result}\"\n",
    "\n",
    "    for allele_name, splits in per_allele_new_models.items():\n",
    "        target_allele_name = allele_name.replace('-', '_').replace('*', '_').replace(':', '_')\n",
    "        allele_path = f\"{base_path}/{target_allele_name}\"\n",
    "        print(\"Allele: \", allele_name)\n",
    "\n",
    "        for split_index, run_model in splits.items():\n",
    "            split_path = f\"{allele_path}/s{split_index}\"\n",
    "            history_per_run = per_allele_histories[allele_name][split_index]\n",
    "            save_visualizations_for_runs(run_model, history_per_run, experiment_params, split_path, predefined_hierarchical_layout=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For multiple alleles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hmm_params = training_parameters.ExperimentParams(experiment_name=\"no_allele_no_split\")\n",
    "hmm_params.model_training_params = training_parameters.SimpleModelClassIIParamsMixMHC()\n",
    "hmm_params.data_scenario_params = training_parameters.PreprocessedIEDBDataParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_params.data_scenario_params.input_data_path = r'C:\\Projects\\grandmaster\\notebooks\\alleles_data\\simple_model_enrichment\\per_length_per_kfold_split'\n",
    "hmm_params.data_scenario_params.splits_to_read = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_params = hmm_params.model_training_params\n",
    "model_training_params.num_runs = 2\n",
    "hmm_params.model_training_params.cycle_chain = True\n",
    "hmm_params.model_training_params.cycle_chain_length = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_params.model_training_params.anchor_top_aas = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will read files from the folder C:\\Projects\\grandmaster\\notebooks\\alleles_data\\simple_model_enrichment\\per_length_per_kfold_split\n",
      "Will read files from the folder C:\\Projects\\grandmaster\\notebooks\\alleles_data\\simple_model_enrichment\\per_length_per_kfold_split\n",
      "['HLA-DRB1*03:01', 'HLA-DRB1*07:01', 'HLA-DRB1*10:01', 'HLA-DRB1*11:01', 'HLA-DRB1*12:01', 'HLA-DRB1*13:03', 'HLA-DRB1*15:01', 'HLA-DRB3*01:01', 'HLA-DRB3*02:02', 'HLA-DRB4*01:01']\n",
      "['HLA-DRB1*03:01', 'HLA-DRB1*07:01', 'HLA-DRB1*10:01', 'HLA-DRB1*11:01', 'HLA-DRB1*12:01', 'HLA-DRB1*13:03', 'HLA-DRB1*15:01', 'HLA-DRB3*01:01', 'HLA-DRB3*02:02', 'HLA-DRB4*01:01']\n"
     ]
    }
   ],
   "source": [
    "per_allele_per_kfold_per_length_binders_train, \\\n",
    "per_allele_per_kfold_per_length_binders_test, additional_data = get_train_test_data(hmm_params)\n",
    "\n",
    "per_allele_per_kfold_per_length_non_binders_train, \\\n",
    "per_allele_per_kfold_per_length_non_binders_test, additional_data = get_train_test_data(hmm_params, \"nonbinders\")\n",
    "\n",
    "\n",
    "PARSED_ALLELES = list(per_allele_per_kfold_per_length_binders_train.keys())\n",
    "PARSED_ALLELES_NB = list(per_allele_per_kfold_per_length_non_binders_train.keys())\n",
    "\n",
    "print(sorted(PARSED_ALLELES))\n",
    "print(sorted(PARSED_ALLELES_NB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_params.lengths_to_use = [12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
    "df = additional_data[0][list(additional_data[0].keys())[0]]\n",
    "[additional_data[0][key] for key in list(additional_data[0].keys())[:3]]\n",
    "current_mix = ['HLA-DRB1*03:01', 'HLA-DRB1*07:01', \n",
    "               'HLA-DRB1*12:01', 'HLA-DRB1*11:01', \n",
    "               'HLA-DRB1*15:01', 'HLA-DRB3*01:01', \n",
    "               'HLA-DRB3*02:02', 'HLA-DRB4*01:01']\n",
    "model_training_params: training_parameters.ModelTrainingParams = hmm_params.model_training_params\n",
    "model_training_params.alleles_to_use = [ item for item in current_mix]\n",
    "\n",
    "#model_training_params.alleles_to_use = [ item for item in PARSED_ALLELES if item in [current_mix]]\n",
    "t = remove_unused_lengths(per_allele_per_kfold_per_length_binders_train, experiment_params=hmm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HLA-DRB1*03:01\n",
      "Length 12: HLA-DRB1*03:01 - 135 \n",
      "Length 13: HLA-DRB1*03:01 - 1555 \n",
      "Length 14: HLA-DRB1*03:01 - 508 \n",
      "Length 15: HLA-DRB1*03:01 - 1240 \n",
      "Length 16: HLA-DRB1*03:01 - 794 \n",
      "Length 17: HLA-DRB1*03:01 - 651 \n",
      "Length 18: HLA-DRB1*03:01 - 460 \n",
      "Length 19: HLA-DRB1*03:01 - 282 \n",
      "Length 20: HLA-DRB1*03:01 - 408 \n",
      "HLA-DRB1*07:01\n",
      "Length 12: HLA-DRB1*07:01 - 163 \n",
      "Length 13: HLA-DRB1*07:01 - 502 \n",
      "Length 14: HLA-DRB1*07:01 - 732 \n",
      "Length 15: HLA-DRB1*07:01 - 3366 \n",
      "Length 16: HLA-DRB1*07:01 - 1000 \n",
      "Length 17: HLA-DRB1*07:01 - 654 \n",
      "Length 18: HLA-DRB1*07:01 - 365 \n",
      "Length 19: HLA-DRB1*07:01 - 199 \n",
      "Length 20: HLA-DRB1*07:01 - 330 \n",
      "HLA-DRB1*11:01\n",
      "Length 12: HLA-DRB1*11:01 - 54 \n",
      "Length 13: HLA-DRB1*11:01 - 281 \n",
      "Length 14: HLA-DRB1*11:01 - 532 \n",
      "Length 15: HLA-DRB1*11:01 - 1634 \n",
      "Length 16: HLA-DRB1*11:01 - 591 \n",
      "Length 17: HLA-DRB1*11:01 - 441 \n",
      "Length 18: HLA-DRB1*11:01 - 310 \n",
      "Length 19: HLA-DRB1*11:01 - 170 \n",
      "Length 20: HLA-DRB1*11:01 - 356 \n",
      "HLA-DRB1*12:01\n",
      "Length 12: HLA-DRB1*12:01 - 116 \n",
      "Length 13: HLA-DRB1*12:01 - 258 \n",
      "Length 14: HLA-DRB1*12:01 - 405 \n",
      "Length 15: HLA-DRB1*12:01 - 653 \n",
      "Length 16: HLA-DRB1*12:01 - 393 \n",
      "Length 17: HLA-DRB1*12:01 - 290 \n",
      "Length 18: HLA-DRB1*12:01 - 194 \n",
      "Length 19: HLA-DRB1*12:01 - 133 \n",
      "Length 20: HLA-DRB1*12:01 - 96 \n",
      "HLA-DRB1*15:01\n",
      "Length 12: HLA-DRB1*15:01 - 159 \n",
      "Length 13: HLA-DRB1*15:01 - 1633 \n",
      "Length 14: HLA-DRB1*15:01 - 624 \n",
      "Length 15: HLA-DRB1*15:01 - 3141 \n",
      "Length 16: HLA-DRB1*15:01 - 758 \n",
      "Length 17: HLA-DRB1*15:01 - 551 \n",
      "Length 18: HLA-DRB1*15:01 - 376 \n",
      "Length 19: HLA-DRB1*15:01 - 263 \n",
      "Length 20: HLA-DRB1*15:01 - 388 \n",
      "HLA-DRB3*01:01\n",
      "Length 12: HLA-DRB3*01:01 - 149 \n",
      "Length 13: HLA-DRB3*01:01 - 279 \n",
      "Length 14: HLA-DRB3*01:01 - 387 \n",
      "Length 15: HLA-DRB3*01:01 - 715 \n",
      "Length 16: HLA-DRB3*01:01 - 387 \n",
      "Length 17: HLA-DRB3*01:01 - 299 \n",
      "Length 18: HLA-DRB3*01:01 - 208 \n",
      "Length 19: HLA-DRB3*01:01 - 134 \n",
      "Length 20: HLA-DRB3*01:01 - 81 \n",
      "HLA-DRB3*02:02\n",
      "Length 12: HLA-DRB3*02:02 - 76 \n",
      "Length 13: HLA-DRB3*02:02 - 276 \n",
      "Length 14: HLA-DRB3*02:02 - 512 \n",
      "Length 15: HLA-DRB3*02:02 - 878 \n",
      "Length 16: HLA-DRB3*02:02 - 725 \n",
      "Length 17: HLA-DRB3*02:02 - 604 \n",
      "Length 18: HLA-DRB3*02:02 - 401 \n",
      "Length 19: HLA-DRB3*02:02 - 264 \n",
      "Length 20: HLA-DRB3*02:02 - 180 \n",
      "HLA-DRB4*01:01\n",
      "Length 12: HLA-DRB4*01:01 - 17 \n",
      "Length 13: HLA-DRB4*01:01 - 83 \n",
      "Length 14: HLA-DRB4*01:01 - 100 \n",
      "Length 15: HLA-DRB4*01:01 - 771 \n",
      "Length 16: HLA-DRB4*01:01 - 137 \n",
      "Length 17: HLA-DRB4*01:01 - 94 \n",
      "Length 18: HLA-DRB4*01:01 - 81 \n",
      "Length 19: HLA-DRB4*01:01 - 46 \n",
      "Length 20: HLA-DRB4*01:01 - 53 \n"
     ]
    }
   ],
   "source": [
    "per_allele_per_kfold_per_length_binders_train = remove_unused_lengths(per_allele_per_kfold_per_length_binders_train, experiment_params=hmm_params)\n",
    "per_allele_per_kfold_per_length_binders_test = remove_unused_lengths(per_allele_per_kfold_per_length_binders_test,experiment_params=hmm_params)\n",
    "\n",
    "# transform to properties if needed and join multiple alleles\n",
    "train_data_b, test_data_b, NEW_ALLELES, \\\n",
    "old_train_data_b, old_test_data_b, OLD_ALLELES = transform_data_to_properties_and_join_alleles(\n",
    "    per_allele_per_kfold_per_length_binders_train,\n",
    "    per_allele_per_kfold_per_length_binders_test,\n",
    "    hmm_params.model_training_params\n",
    ")\n",
    "# Calculate weights for the training data based on peptide couns/lengths for unmerged data\n",
    "train_data_weigths_b = calculate_weights_based_on_length_counts(old_train_data_b, experiment_params=hmm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HLA-DRB1*03:01\n",
      "Length 12: HLA-DRB1*03:01 - 63 \n",
      "Length 13: HLA-DRB1*03:01 - 61656 \n",
      "Length 14: HLA-DRB1*03:01 - 120 \n",
      "Length 15: HLA-DRB1*03:01 - 1794 \n",
      "Length 16: HLA-DRB1*03:01 - 113 \n",
      "Length 17: HLA-DRB1*03:01 - 75 \n",
      "Length 18: HLA-DRB1*03:01 - 68 \n",
      "Length 19: HLA-DRB1*03:01 - 50 \n",
      "Length 20: HLA-DRB1*03:01 - 505 \n",
      "HLA-DRB1*07:01\n",
      "Length 12: HLA-DRB1*07:01 - 55 \n",
      "Length 13: HLA-DRB1*07:01 - 76 \n",
      "Length 14: HLA-DRB1*07:01 - 54 \n",
      "Length 15: HLA-DRB1*07:01 - 8864 \n",
      "Length 16: HLA-DRB1*07:01 - 126 \n",
      "Length 17: HLA-DRB1*07:01 - 85 \n",
      "Length 18: HLA-DRB1*07:01 - 76 \n",
      "Length 19: HLA-DRB1*07:01 - 31 \n",
      "Length 20: HLA-DRB1*07:01 - 410 \n",
      "HLA-DRB1*11:01\n",
      "Length 12: HLA-DRB1*11:01 - 53 \n",
      "Length 13: HLA-DRB1*11:01 - 104 \n",
      "Length 14: HLA-DRB1*11:01 - 51 \n",
      "Length 15: HLA-DRB1*11:01 - 1871 \n",
      "Length 16: HLA-DRB1*11:01 - 104 \n",
      "Length 17: HLA-DRB1*11:01 - 69 \n",
      "Length 18: HLA-DRB1*11:01 - 65 \n",
      "Length 19: HLA-DRB1*11:01 - 19 \n",
      "Length 20: HLA-DRB1*11:01 - 350 \n",
      "HLA-DRB1*12:01\n",
      "Length 12: HLA-DRB1*12:01 - 0 \n",
      "Length 13: HLA-DRB1*12:01 - 0 \n",
      "Length 14: HLA-DRB1*12:01 - 34 \n",
      "Length 15: HLA-DRB1*12:01 - 864 \n",
      "Length 16: HLA-DRB1*12:01 - 13 \n",
      "Length 17: HLA-DRB1*12:01 - 0 \n",
      "Length 18: HLA-DRB1*12:01 - 0 \n",
      "Length 19: HLA-DRB1*12:01 - 0 \n",
      "Length 20: HLA-DRB1*12:01 - 18 \n",
      "HLA-DRB1*15:01\n",
      "Length 12: HLA-DRB1*15:01 - 51 \n",
      "Length 13: HLA-DRB1*15:01 - 61651 \n",
      "Length 14: HLA-DRB1*15:01 - 43 \n",
      "Length 15: HLA-DRB1*15:01 - 9115 \n",
      "Length 16: HLA-DRB1*15:01 - 92 \n",
      "Length 17: HLA-DRB1*15:01 - 79 \n",
      "Length 18: HLA-DRB1*15:01 - 55 \n",
      "Length 19: HLA-DRB1*15:01 - 17 \n",
      "Length 20: HLA-DRB1*15:01 - 332 \n",
      "HLA-DRB3*01:01\n",
      "Length 12: HLA-DRB3*01:01 - 45 \n",
      "Length 13: HLA-DRB3*01:01 - 58 \n",
      "Length 14: HLA-DRB3*01:01 - 7 \n",
      "Length 15: HLA-DRB3*01:01 - 1407 \n",
      "Length 16: HLA-DRB3*01:01 - 49 \n",
      "Length 17: HLA-DRB3*01:01 - 38 \n",
      "Length 18: HLA-DRB3*01:01 - 28 \n",
      "Length 19: HLA-DRB3*01:01 - 0 \n",
      "Length 20: HLA-DRB3*01:01 - 36 \n",
      "HLA-DRB3*02:02\n",
      "Length 12: HLA-DRB3*02:02 - 0 \n",
      "Length 13: HLA-DRB3*02:02 - 0 \n",
      "Length 14: HLA-DRB3*02:02 - 0 \n",
      "Length 15: HLA-DRB3*02:02 - 860 \n",
      "Length 16: HLA-DRB3*02:02 - 0 \n",
      "Length 17: HLA-DRB3*02:02 - 0 \n",
      "Length 18: HLA-DRB3*02:02 - 0 \n",
      "Length 19: HLA-DRB3*02:02 - 0 \n",
      "Length 20: HLA-DRB3*02:02 - 0 \n",
      "HLA-DRB4*01:01\n",
      "Length 12: HLA-DRB4*01:01 - 9 \n",
      "Length 13: HLA-DRB4*01:01 - 99 \n",
      "Length 14: HLA-DRB4*01:01 - 0 \n",
      "Length 15: HLA-DRB4*01:01 - 1223 \n",
      "Length 16: HLA-DRB4*01:01 - 49 \n",
      "Length 17: HLA-DRB4*01:01 - 38 \n",
      "Length 18: HLA-DRB4*01:01 - 31 \n",
      "Length 19: HLA-DRB4*01:01 - 0 \n",
      "Length 20: HLA-DRB4*01:01 - 73 \n"
     ]
    }
   ],
   "source": [
    "per_allele_per_kfold_per_length_non_binders_train = remove_unused_lengths(per_allele_per_kfold_per_length_non_binders_train, experiment_params=hmm_params)\n",
    "per_allele_per_kfold_per_length_non_binders_test = remove_unused_lengths(per_allele_per_kfold_per_length_non_binders_test,experiment_params=hmm_params)\n",
    "\n",
    "# transform to properties if needed and join multiple alleles\n",
    "train_data_nb, test_data_nb, NEW_ALLELES, \\\n",
    "old_train_data_nb, old_test_data_nb, OLD_ALLELES = transform_data_to_properties_and_join_alleles(\n",
    "    per_allele_per_kfold_per_length_non_binders_train,\n",
    "    per_allele_per_kfold_per_length_non_binders_test,\n",
    "    hmm_params.model_training_params\n",
    ")\n",
    "# Calculate weights for the training data based on peptide couns/lengths for unmerged data\n",
    "train_data_weigths_nb = calculate_weights_based_on_length_counts(old_train_data_nb, experiment_params=hmm_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(10)\n",
    "\n",
    "from hmm_logic_methods import train_model_prepared,  \\\n",
    "    train_model_batched, save_model, \\\n",
    "    add_more_states_and_reset_transitions, \\\n",
    "    build_model_based_on_params, \\\n",
    "    train_multiple_models, \\\n",
    "reorder_models_by_score_and_flatten_to_by_name_list, hierarchically_train_splited_models, load_model\n",
    "from hmm_visualization_methods import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_allele_per_run_per_split_prepared_models_binders = prepare_multiple_models(hmm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_params.experiment_result_data_path = 'C:\\\\Projects\\\\grandmaster\\\\notebooks\\\\MHC_predictor\\\\experiments\\\\core_identification_simple_model_enrichment\\\\experiment_results\\\\mult\\\\anc7' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allele HLA-DRB1*03:01: Split 0 Run root[0000] Run root[0001]  \n",
      "Split 1 Run root[0000] Run root[0001]  \n",
      "Allele HLA-DRB1*07:01: Split 0 Run root[0000] Run root[0001]  \n",
      "Split 1 Run root[0000] Run root[0001]  \n",
      "Allele HLA-DRB1*11:01: Split 0 Run root[0000] Run root[0001]  \n",
      "Split 1 Run root[0000] Run root[0001]  \n",
      "Allele HLA-DRB1*12:01: Split 0 Run root[0000] Run root[0001]  \n",
      "Split 1 Run root[0000] Run root[0001]  \n",
      "Allele HLA-DRB1*15:01: Split 0 Run root[0000] Run root[0001]  \n",
      "Split 1 Run root[0000] Run root[0001]  \n",
      "Allele HLA-DRB3*01:01: Split 0 Run root[0000] Run root[0001]  \n",
      "Split 1 Run root[0000] Run root[0001]  \n",
      "Allele HLA-DRB3*02:02: Split 0 Run root[0000] Run root[0001]  \n",
      "Split 1 Run root[0000] Run root[0001]  \n",
      "Allele HLA-DRB4*01:01: Split 0 Run root[0000] Run root[0001]  \n",
      "Split 1 Run root[0000] Run root[0001]  \n"
     ]
    }
   ],
   "source": [
    "result_models_b, result_histories_b =  train_models_for_alleles(\n",
    "    prepared_models=per_allele_per_run_per_split_prepared_models_binders,\n",
    "    train_data=train_data_b,\n",
    "    test_data=test_data_b,\n",
    "    train_data_weights=train_data_weigths_b,\n",
    "    experiment_params=hmm_params,\n",
    "    subfolder_to_save_result=\"binders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "non binders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_allele_per_run_per_split_prepared_models_nonbinders = prepare_multiple_models(hmm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allele HLA-DRB1*03:01: Split 0 Run root[0000] Run root[0001]  \n",
      "Split 1 Run root[0000] Run root[0001]  \n",
      "Allele HLA-DRB1*07:01: Split 0 Run root[0000] Run root[0001]  \n",
      "Split 1 Run root[0000] Run root[0001]  \n",
      "Allele HLA-DRB1*11:01: Split 0 Run root[0000] Run root[0001]  \n",
      "Split 1 Run root[0000] Run root[0001]  \n",
      "Allele HLA-DRB1*12:01: Split 0 Run root[0000] Run root[0001]  \n",
      "Split 1 Run root[0000] Run root[0001]  \n",
      "Allele HLA-DRB1*15:01: Split 0 Run root[0000] Run root[0001]  \n",
      "Split 1 Run root[0000] Run root[0001]  \n",
      "Allele HLA-DRB3*01:01: Split 0 Run root[0000] Run root[0001]  \n",
      "Split 1 Run root[0000] Run root[0001]  \n",
      "Allele HLA-DRB3*02:02: Split 0 Run root[0000] Run root[0001]  \n",
      "Split 1 Run root[0000] Run root[0001]  \n",
      "Allele HLA-DRB4*01:01: Split 0 Run root[0000] Run root[0001]  \n",
      "Split 1 Run root[0000] Run root[0001]  \n"
     ]
    }
   ],
   "source": [
    "result_models_nb, result_histories_nb =  train_models_for_alleles(\n",
    "    prepared_models=per_allele_per_run_per_split_prepared_models_nonbinders,\n",
    "    train_data=train_data_nb,\n",
    "    test_data=test_data_nb,\n",
    "    train_data_weights=train_data_weigths_nb,\n",
    "    experiment_params=hmm_params,\n",
    "    subfolder_to_save_result=\"nonbinders\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pomegranate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
